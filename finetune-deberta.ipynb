{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11285711,"sourceType":"datasetVersion","datasetId":7056277},{"sourceId":11287711,"sourceType":"datasetVersion","datasetId":7057682},{"sourceId":11287739,"sourceType":"datasetVersion","datasetId":7057704},{"sourceId":11293087,"sourceType":"datasetVersion","datasetId":7061218},{"sourceId":11295344,"sourceType":"datasetVersion","datasetId":7062895}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable wandb logs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:15:17.249819Z","iopub.execute_input":"2025-04-06T09:15:17.250089Z","iopub.status.idle":"2025-04-06T09:15:17.254440Z","shell.execute_reply.started":"2025-04-06T09:15:17.250060Z","shell.execute_reply":"2025-04-06T09:15:17.253601Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:15:17.255300Z","iopub.execute_input":"2025-04-06T09:15:17.255651Z","iopub.status.idle":"2025-04-06T09:15:21.629278Z","shell.execute_reply.started":"2025-04-06T09:15:17.255619Z","shell.execute_reply":"2025-04-06T09:15:21.628192Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/final-18k-dataset/cleaned_shuffled_merged_dataset.csv')  # change path if needed\nprint(df.head())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:16:17.663970Z","iopub.execute_input":"2025-04-06T09:16:17.664450Z","iopub.status.idle":"2025-04-06T09:16:17.876047Z","shell.execute_reply.started":"2025-04-06T09:16:17.664424Z","shell.execute_reply":"2025-04-06T09:16:17.875133Z"}},"outputs":[{"name":"stdout","text":"                     filename  \\\n0         audio_668_aug29.wav   \n1        audio_478_aug677.wav   \n2         audio_947_aug60.wav   \n3  audio_942_aug46_aug943.wav   \n4       audio_445_aug1209.wav   \n\n                                       transcription  label  \n0  there follow so many people in the lane like v...    2.5  \n1  i will pass through your house. i will let him...    4.5  \n2  from a fan created very huge in circular area ...    2.0  \n3  a playground looks above us clear and neat as ...    1.5  \n4  my family playground were not really big. it w...    3.0  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"df[\"label\"].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:16:20.791006Z","iopub.execute_input":"2025-04-06T09:16:20.791309Z","iopub.status.idle":"2025-04-06T09:16:20.797267Z","shell.execute_reply.started":"2025-04-06T09:16:20.791278Z","shell.execute_reply":"2025-04-06T09:16:20.796439Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([2.5, 4.5, 2. , 1.5, 3. , 4. , 3.5, 5. , 1. ])"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load CSV (update path as needed)\n\n\n# Check and clean\ndf = df[['transcription', 'label']].dropna()\n\n# Convert float labels to string to preserve classes like '2.5', '3.5', etc.\ndf['label'] = df['label'].astype(str)\n\n# Encode labels (e.g., '1.0' -> 0, '1.5' -> 1, ..., '5.0' -> 8)\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['label'])\n\n# Preview\nprint(df.head())\nprint(\"Label classes:\", list(le.classes_))  # You can store this for decoding later\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:15:44.038895Z","iopub.execute_input":"2025-04-06T09:15:44.039170Z","iopub.status.idle":"2025-04-06T09:15:44.508462Z","shell.execute_reply.started":"2025-04-06T09:15:44.039150Z","shell.execute_reply":"2025-04-06T09:15:44.507740Z"}},"outputs":[{"name":"stdout","text":"                                       transcription  label\n0  it are so in all estimated. presentat gasoline...      6\n1  The Del mercad trotzdem –≤–ørago Chshh Ill leave...      8\n2  i value botany, i continuously gain immense of...      0\n3  my favorite place is to visit mecca and medina...      5\n4  Im in a public market in the Philippines and a...      7\nLabel classes: ['1.0', '1.5', '2.0', '2.5', '3.0', '3.5', '4.0', '4.5', '5.0']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datasets import Dataset\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_pandas(df)\n\n# Split into train/test\ndataset = dataset.train_test_split(test_size=0.01, seed=42)\n\n# Optional: inspect a sample\nprint(dataset['train'][0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:15:50.086008Z","iopub.execute_input":"2025-04-06T09:15:50.086479Z","iopub.status.idle":"2025-04-06T09:15:51.647409Z","shell.execute_reply.started":"2025-04-06T09:15:50.086449Z","shell.execute_reply":"2025-04-06T09:15:51.646680Z"}},"outputs":[{"name":"stdout","text":"{'transcription': 'hobbies are work which a person does no interest gets much satisfaction and amusement. it is a kind of recreation, shatter from the searching rays of the sun, and also get fruit from it. there are many kinds of hobbies and we select one of them here per our will and mindset. some our hobby of collecting post - all, also collecting interesting book of stories, some pet birds but my hobby is gardening. i am much interested in gardening since my childhood. i like to see the flowers, velvary grass, different color flowers and beautiful plants. so i have selected long piece of land in my house and planted different kind of trees. i further planted red rose, yellow and black rose plants which have arranged them one after another.', 'label': 5, '__index_level_0__': 353}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load DeBERTa v3 Large tokenizer\ncheckpoint = \"/kaggle/input/deberta-v3-large-base\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# Tokenization function\ndef tokenize_fn(example):\n    return tokenizer(example[\"transcription\"], truncation=True, padding=\"max_length\", max_length=512)\n\n# Apply to dataset\ntokenized_dataset = dataset.map(tokenize_fn, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:28:36.622913Z","iopub.execute_input":"2025-04-06T08:28:36.623552Z","iopub.status.idle":"2025-04-06T08:28:38.077813Z","shell.execute_reply.started":"2025-04-06T08:28:36.623517Z","shell.execute_reply":"2025-04-06T08:28:38.076788Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/884 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c52802fac67e463aa3a31fb2f004fd47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f45069e2c8e64107962f89e72c5d09d8"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    checkpoint,\n    num_labels=9\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:28:39.001036Z","iopub.execute_input":"2025-04-06T08:28:39.001320Z","iopub.status.idle":"2025-04-06T08:28:45.304594Z","shell.execute_reply.started":"2025-04-06T08:28:39.001300Z","shell.execute_reply":"2025-04-06T08:28:45.303660Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# dont use these cell for finetuning\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"./deberta-v3-large-base\")\ntokenizer.save_pretrained(\"./deberta-v3-large-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T16:03:27.481961Z","iopub.execute_input":"2025-04-05T16:03:27.483090Z","iopub.status.idle":"2025-04-05T16:03:30.379228Z","shell.execute_reply.started":"2025-04-05T16:03:27.483055Z","shell.execute_reply":"2025-04-05T16:03:30.378519Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"('./deberta-v3-large-base/tokenizer_config.json',\n './deberta-v3-large-base/special_tokens_map.json',\n './deberta-v3-large-base/spm.model',\n './deberta-v3-large-base/added_tokens.json',\n './deberta-v3-large-base/tokenizer.json')"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import os\nimport json\n\ndataset_name = \"deberta-v3-large-base\"\nkaggle_username = \"avinash1tiwari\"  # üîÅ Replace this manually!\n\n# Metadata content\nmetadata = {\n    \"title\": dataset_name,\n    \"id\": f\"{kaggle_username}/{dataset_name}\",\n    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}\n\n# Save metadata file\nwith open(os.path.join(dataset_name, \"dataset-metadata.json\"), \"w\") as f:\n    json.dump(metadata, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T16:14:43.935058Z","iopub.execute_input":"2025-04-05T16:14:43.935558Z","iopub.status.idle":"2025-04-05T16:14:43.940698Z","shell.execute_reply.started":"2025-04-05T16:14:43.935517Z","shell.execute_reply":"2025-04-05T16:14:43.939939Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"!mkdir -p ~/.kaggle\nwith open(\"/kaggle/input/kaggle-api/kaggle.json\", \"r\") as src:\n    with open(\"/root/.kaggle/kaggle.json\", \"w\") as dst:\n        dst.write(src.read())\n!chmod 600 /root/.kaggle/kaggle.json\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T16:18:08.811865Z","iopub.execute_input":"2025-04-05T16:18:08.812223Z","iopub.status.idle":"2025-04-05T16:18:09.165847Z","shell.execute_reply.started":"2025-04-05T16:18:08.812161Z","shell.execute_reply":"2025-04-05T16:18:09.164631Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"!kaggle datasets create -p deberta-v3-large-base --dir-mode zip\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T16:18:40.128956Z","iopub.execute_input":"2025-04-05T16:18:40.129341Z","iopub.status.idle":"2025-04-05T16:20:00.834436Z","shell.execute_reply.started":"2025-04-05T16:18:40.129308Z","shell.execute_reply":"2025-04-05T16:20:00.833293Z"}},"outputs":[{"name":"stdout","text":"Starting upload for file config.json\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.25k/1.25k [00:00<00:00, 1.39kB/s]\nUpload successful: config.json (1KB)\nStarting upload for file added_tokens.json\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23.0/23.0 [00:01<00:00, 13.8B/s]\nUpload successful: added_tokens.json (23B)\nStarting upload for file special_tokens_map.json\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 286/286 [00:00<00:00, 330B/s]\nUpload successful: special_tokens_map.json (286B)\nStarting upload for file tokenizer_config.json\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.28k/1.28k [00:01<00:00, 779B/s]\nUpload successful: tokenizer_config.json (1KB)\nStarting upload for file tokenizer.json\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.26M/8.26M [00:03<00:00, 2.71MB/s]\nUpload successful: tokenizer.json (8MB)\nStarting upload for file spm.model\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.35M/2.35M [00:02<00:00, 833kB/s]\nUpload successful: spm.model (2MB)\nStarting upload for file model.safetensors\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.62G/1.62G [01:03<00:00, 27.2MB/s]\nUpload successful: model.safetensors (2GB)\nYour private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/avinash1tiwari/deberta-v3-large-base\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# start finetuning code from here\n","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:28:47.239273Z","iopub.execute_input":"2025-04-06T08:28:47.240000Z","iopub.status.idle":"2025-04-06T08:28:47.285820Z","shell.execute_reply.started":"2025-04-06T08:28:47.239968Z","shell.execute_reply":"2025-04-06T08:28:47.285144Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:28:49.507320Z","iopub.execute_input":"2025-04-06T08:28:49.507607Z","iopub.status.idle":"2025-04-06T08:28:53.358611Z","shell.execute_reply.started":"2025-04-06T08:28:49.507583Z","shell.execute_reply":"2025-04-06T08:28:53.357594Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import evaluate\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    return accuracy.compute(predictions=predictions, references=labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:28:53.359891Z","iopub.execute_input":"2025-04-06T08:28:53.360108Z","iopub.status.idle":"2025-04-06T08:28:55.014209Z","shell.execute_reply.started":"2025-04-06T08:28:53.360089Z","shell.execute_reply":"2025-04-06T08:28:55.013561Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"750bc7a9c917454d9b61da72e6deac0d"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./deberta-v3-large-finetuned(700-5epochs)\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    learning_rate=2e-5,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,  # Simulates effective batch size of 4\n    num_train_epochs=5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",  # or \"f1\", depending on what compute_metrics returns\n    save_total_limit=1,\n    push_to_hub=False,\n    fp16=True  # Most important memory-saving flag\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:28:56.484195Z","iopub.execute_input":"2025-04-06T08:28:56.484478Z","iopub.status.idle":"2025-04-06T08:28:57.554182Z","shell.execute_reply.started":"2025-04-06T08:28:56.484458Z","shell.execute_reply":"2025-04-06T08:28:57.553237Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:29:02.005523Z","iopub.execute_input":"2025-04-06T08:29:02.005868Z","iopub.status.idle":"2025-04-06T08:29:19.656980Z","shell.execute_reply.started":"2025-04-06T08:29:02.005840Z","shell.execute_reply":"2025-04-06T08:29:19.656298Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-14-410e4f14925b>:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:28:00.448380Z","iopub.execute_input":"2025-04-06T07:28:00.448702Z","iopub.status.idle":"2025-04-06T07:28:00.453777Z","shell.execute_reply.started":"2025-04-06T07:28:00.448679Z","shell.execute_reply":"2025-04-06T07:28:00.452876Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(893, 2)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:29:19.657948Z","iopub.execute_input":"2025-04-06T08:29:19.658151Z","iopub.status.idle":"2025-04-06T08:55:02.779445Z","shell.execute_reply.started":"2025-04-06T08:29:19.658133Z","shell.execute_reply":"2025-04-06T08:55:02.778553Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1105' max='1105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1105/1105 25:39, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.190900</td>\n      <td>1.613279</td>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.966900</td>\n      <td>1.413299</td>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.797600</td>\n      <td>1.427943</td>\n      <td>0.444444</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.387300</td>\n      <td>1.336622</td>\n      <td>0.444444</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.210400</td>\n      <td>1.775167</td>\n      <td>0.333333</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1105, training_loss=0.9040224822937634, metrics={'train_runtime': 1541.6539, 'train_samples_per_second': 2.867, 'train_steps_per_second': 0.717, 'total_flos': 4119261858017280.0, 'train_loss': 0.9040224822937634, 'epoch': 5.0})"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/deberta-v3-large-finetuned-700-5epochs\")\ntokenizer.save_pretrained(\"/kaggle/working/deberta-v3-large-finetuned-700-5epochs\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:00:35.122249Z","iopub.execute_input":"2025-04-06T09:00:35.122570Z","iopub.status.idle":"2025-04-06T09:00:39.320366Z","shell.execute_reply.started":"2025-04-06T09:00:35.122546Z","shell.execute_reply":"2025-04-06T09:00:39.319568Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/deberta-v3-large-finetuned-700-5epochs/tokenizer_config.json',\n '/kaggle/working/deberta-v3-large-finetuned-700-5epochs/special_tokens_map.json',\n '/kaggle/working/deberta-v3-large-finetuned-700-5epochs/spm.model',\n '/kaggle/working/deberta-v3-large-finetuned-700-5epochs/added_tokens.json',\n '/kaggle/working/deberta-v3-large-finetuned-700-5epochs/tokenizer.json')"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"# save the finetuned data just by running this cell\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle.api.kaggle_api_extended import KaggleApi\nimport json\n\n# Set Kaggle credentials as environment variables\nos.environ['KAGGLE_USERNAME'] = \"avinash1tiwari\"\nos.environ['KAGGLE_KEY'] = \"21a0b93c002942f2e649b292681fc4cf\"\n\n# Set the directory and dataset name (renaming folder to avoid parentheses)\nmodel_dir = \"/kaggle/working/deberta-v3-large-finetuned-700-5epochs\"  # renamed folder name\ndataset_name = \"deberta-v3-large-finetuned-700-5epochs\"  # renamed dataset name\ndataset_id = f\"avinash1tiwari/{dataset_name}\"\n\n# Create metadata file in the directory\nmetadata = {\n    \"title\": dataset_name,\n    \"id\": dataset_id,\n    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}\n\n# Save the metadata to a JSON file\nwith open(os.path.join(model_dir, \"dataset-metadata.json\"), \"w\") as f:\n    json.dump(metadata, f)\n\n# Authenticate with Kaggle API\napi = KaggleApi()\napi.authenticate()\n\n# Create a new dataset (this just creates the dataset on Kaggle)\napi.dataset_create_new(folder=model_dir, public=False)\n\n# Now upload the model directory with subdirectories (use kaggle CLI command)\n# Run the shell command to upload the folder using Kaggle's CLI tool\n!kaggle datasets create -p {model_dir} --dir-mode\n\nprint(\"‚úÖ Model uploaded successfully to Kaggle Datasets.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:01:07.892641Z","iopub.execute_input":"2025-04-06T09:01:07.892989Z","iopub.status.idle":"2025-04-06T09:02:26.322126Z","shell.execute_reply.started":"2025-04-06T09:01:07.892965Z","shell.execute_reply":"2025-04-06T09:02:26.321197Z"}},"outputs":[{"name":"stdout","text":"Starting upload for file added_tokens.json\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23.0/23.0 [00:00<00:00, 26.7B/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: added_tokens.json (23B)\nStarting upload for file special_tokens_map.json\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 970/970 [00:00<00:00, 1.14kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: special_tokens_map.json (970B)\nStarting upload for file config.json\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.26k/1.26k [00:00<00:00, 1.42kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: config.json (1KB)\nStarting upload for file spm.model\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.35M/2.35M [00:02<00:00, 1.17MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: spm.model (2MB)\nStarting upload for file model.safetensors\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.62G/1.62G [01:04<00:00, 27.0MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Upload successful: model.safetensors (2GB)\nStarting upload for file tokenizer_config.json\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.47k/1.47k [00:00<00:00, 1.76kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: tokenizer_config.json (1KB)\nStarting upload for file tokenizer.json\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.25M/8.25M [00:02<00:00, 3.68MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Upload successful: tokenizer.json (8MB)\nusage: kaggle datasets create [-h] [-p FOLDER] [-u] [-q] [-t] [-r {skip,zip,tar}]\nkaggle datasets create: error: argument -r/--dir-mode: expected one argument\n‚úÖ Model uploaded successfully to Kaggle Datasets.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Total Parameters: {total_params/1e6:.2f}M\")\nprint(f\"Trainable Parameters: {trainable_params/1e6:.2f}M\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T06:42:07.523605Z","iopub.execute_input":"2025-04-06T06:42:07.523968Z","iopub.status.idle":"2025-04-06T06:42:07.531707Z","shell.execute_reply.started":"2025-04-06T06:42:07.523927Z","shell.execute_reply":"2025-04-06T06:42:07.530883Z"}},"outputs":[{"name":"stdout","text":"Total Parameters: 435.07M\nTrainable Parameters: 435.07M\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/shl-dataset/transcribed.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:11:10.600591Z","iopub.execute_input":"2025-04-05T14:11:10.600905Z","iopub.status.idle":"2025-04-05T14:11:10.626099Z","shell.execute_reply.started":"2025-04-05T14:11:10.600878Z","shell.execute_reply":"2025-04-05T14:11:10.625438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nprint(\"üîπ First 10 rows of the dataset:\")\nprint(df.head(10))\n\nprint(\"\\nüîπ Dataset info:\")\nprint(df.info())\n\nprint(\"\\nüîπ Missing values per column:\")\nprint(df.isnull().sum())\n\nprint(\"\\nüîπ Label value counts (encoded):\")\nprint(df['label'].value_counts().sort_index())\n\nprint(\"\\nüîπ Unique labels (encoded):\", df['label'].nunique())\nprint(\"üîπ Total rows:\", len(df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T14:11:19.278839Z","iopub.execute_input":"2025-04-05T14:11:19.279178Z","iopub.status.idle":"2025-04-05T14:11:19.298760Z","shell.execute_reply.started":"2025-04-05T14:11:19.279150Z","shell.execute_reply":"2025-04-05T14:11:19.298030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\nfrom scipy.stats import pearsonr\nimport numpy as np\n\n# Get predictions on train set\ntrain_predictions = trainer.predict(tokenized_dataset[\"train\"])\ny_pred = np.argmax(train_predictions.predictions, axis=1)\ny_true = train_predictions.label_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:28:38.585747Z","iopub.execute_input":"2025-04-06T07:28:38.586082Z","iopub.status.idle":"2025-04-06T07:30:16.598703Z","shell.execute_reply.started":"2025-04-06T07:28:38.586033Z","shell.execute_reply":"2025-04-06T07:30:16.598031Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"corr, _ = pearsonr(y_true, y_pred)\nprint(f\"üìä Pearson Correlation Coefficient (Train Set): {corr:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:28:28.531264Z","iopub.execute_input":"2025-04-06T07:28:28.531577Z","iopub.status.idle":"2025-04-06T07:28:28.543149Z","shell.execute_reply.started":"2025-04-06T07:28:28.531549Z","shell.execute_reply":"2025-04-06T07:28:28.542330Z"}},"outputs":[{"name":"stdout","text":"üìä Pearson Correlation Coefficient (Train Set): 0.9408\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"output_dir = \"/kaggle/working/deberta-v3-large-finetuned\"\n\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T15:27:45.607661Z","iopub.execute_input":"2025-04-05T15:27:45.607940Z","iopub.status.idle":"2025-04-05T15:27:50.719824Z","shell.execute_reply.started":"2025-04-05T15:27:45.607918Z","shell.execute_reply":"2025-04-05T15:27:50.718801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\noutput_dir = \"/kaggle/working/deberta-v3-large-finetuned\"\nprint(\"Files inside:\", os.listdir(output_dir))\n\n# Check file sizes\nfor f in os.listdir(output_dir):\n    path = os.path.join(output_dir, f)\n    print(f\"{f} ‚Äî {round(os.path.getsize(path)/1024/1024, 2)} MB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T15:34:31.211437Z","iopub.execute_input":"2025-04-05T15:34:31.211755Z","iopub.status.idle":"2025-04-05T15:34:31.218782Z","shell.execute_reply.started":"2025-04-05T15:34:31.211731Z","shell.execute_reply":"2025-04-05T15:34:31.218061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}