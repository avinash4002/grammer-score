{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11288705,"sourceType":"datasetVersion","datasetId":7058440},{"sourceId":11288974,"sourceType":"datasetVersion","datasetId":7058632},{"sourceId":11289471,"sourceType":"datasetVersion","datasetId":7058987},{"sourceId":11301895,"sourceType":"datasetVersion","datasetId":7067988},{"sourceId":11302078,"sourceType":"datasetVersion","datasetId":7068138},{"sourceId":11302214,"sourceType":"datasetVersion","datasetId":7068233},{"sourceId":11302436,"sourceType":"datasetVersion","datasetId":7068396},{"sourceId":11302557,"sourceType":"datasetVersion","datasetId":7068484},{"sourceId":11302641,"sourceType":"datasetVersion","datasetId":7068542}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:12:55.476355Z","iopub.execute_input":"2025-04-07T00:12:55.476542Z","iopub.status.idle":"2025-04-07T00:12:57.076395Z","shell.execute_reply.started":"2025-04-07T00:12:55.476523Z","shell.execute_reply":"2025-04-07T00:12:57.075507Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport torch.nn.functional as F\n\n# Load tokenizer and model from the dataset path\nmodel_path = \"/kaggle/input/deberta-v3-large-gptlastoptimisedepoch3\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n\n# Ensure model is in evaluation mode\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:12:57.077255Z","iopub.execute_input":"2025-04-07T00:12:57.077712Z","iopub.status.idle":"2025-04-07T00:13:26.630571Z","shell.execute_reply.started":"2025-04-07T00:12:57.077689Z","shell.execute_reply":"2025-04-07T00:13:26.629716Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DebertaV2ForSequenceClassification(\n  (deberta): DebertaV2Model(\n    (embeddings): DebertaV2Embeddings(\n      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaV2Encoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaV2Layer(\n          (attention): DebertaV2Attention(\n            (self): DisentangledSelfAttention(\n              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaV2SelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaV2Intermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaV2Output(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(512, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=9, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:13:26.634492Z","iopub.execute_input":"2025-04-07T00:13:26.634786Z","iopub.status.idle":"2025-04-07T00:13:26.718929Z","shell.execute_reply.started":"2025-04-07T00:13:26.634758Z","shell.execute_reply":"2025-04-07T00:13:26.717953Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:13:26.719924Z","iopub.execute_input":"2025-04-07T00:13:26.720278Z","iopub.status.idle":"2025-04-07T00:13:39.973870Z","shell.execute_reply.started":"2025-04-07T00:13:26.720245Z","shell.execute_reply":"2025-04-07T00:13:39.973013Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DebertaV2ForSequenceClassification(\n  (deberta): DebertaV2Model(\n    (embeddings): DebertaV2Embeddings(\n      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaV2Encoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaV2Layer(\n          (attention): DebertaV2Attention(\n            (self): DisentangledSelfAttention(\n              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaV2SelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaV2Intermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaV2Output(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(512, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=1024, out_features=9, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"id_to_label = {\n    0: '1.0',\n    1: '1.5',\n    2: '2.0',\n    3: '2.5',\n    4: '3.0',\n    5: '3.5',\n    6: '4.0',\n    7: '4.5',\n    8: '5.0'\n}\n\n# Load your CSV (update path if needed)\ninput_csv_path = \"/kaggle/input/test-transcription/transcribed_test.csv\"\ndf = pd.read_csv(input_csv_path)\n\n# Make sure the relevant columns exist\nassert 'filename' in df.columns and 'transcription' in df.columns, \"Missing required columns.\"\n\n# Store predictions\nresults = []\n\n# Inference loop\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:13:39.974746Z","iopub.execute_input":"2025-04-07T00:13:39.975018Z","iopub.status.idle":"2025-04-07T00:13:40.012749Z","shell.execute_reply.started":"2025-04-07T00:13:39.974988Z","shell.execute_reply":"2025-04-07T00:13:40.012133Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Inference loop\nfor idx, row in tqdm(df.iterrows(), total=len(df)):\n    text = row['transcription']\n    filename = row['filename']\n\n    # Fallback for invalid transcription\n    if not isinstance(text, str) or text.strip() == \"\":\n        results.append({\n            \"filename\": filename,\n            \"predicted_label\": 5  # Fallback label for invalid input\n        })\n        continue\n\n    try:\n        # Tokenize and predict\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n        with torch.no_grad():\n            logits = model(**inputs).logits\n            probs = F.softmax(logits, dim=1)\n            pred_class = torch.argmax(probs, dim=1).item()\n            label = id_to_label[pred_class]\n\n        results.append({\n            \"filename\": filename,\n            \"predicted_label\": label\n        })\n\n    except Exception as e:\n        print(f\"Error on row {idx}: {e}. Assigning fallback label 5.\")\n        results.append({\n            \"filename\": filename,\n            \"predicted_label\": 5  # Also use fallback in case of unexpected error\n        })\n\n# Save predictions\noutput_df = pd.DataFrame(results)\noutput_df.to_csv(\"/kaggle/working/predicted_labels1.csv\", index=False)\n\nprint(\"✅ Predictions saved to /kaggle/working/predicted_labels.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:14:11.421942Z","iopub.execute_input":"2025-04-07T00:14:11.422271Z","iopub.status.idle":"2025-04-07T00:14:23.914546Z","shell.execute_reply.started":"2025-04-07T00:14:11.422244Z","shell.execute_reply":"2025-04-07T00:14:23.913713Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/195 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n100%|██████████| 195/195 [00:12<00:00, 15.63it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Predictions saved to /kaggle/working/predicted_labels.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}